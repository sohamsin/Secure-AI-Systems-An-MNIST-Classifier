{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3e28c0b8-0096-4e6b-b9ad-b93cc5a6d145","_cell_guid":"c968d9ad-bfaf-4a4f-92e4-6d2084795a1a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:19:49.604166Z","iopub.execute_input":"2025-11-16T18:19:49.604862Z","iopub.status.idle":"2025-11-16T18:19:49.926447Z","shell.execute_reply.started":"2025-11-16T18:19:49.604835Z","shell.execute_reply":"2025-11-16T18:19:49.925820Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 1: Setup and Imports\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Create output directory for Kaggle\nos.makedirs('/kaggle/working/outputs', exist_ok=True)\nOUTPUT_DIR = '/kaggle/working/outputs/'\n\n# Set plot style for better visualization\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Check TensorFlow version and GPU availability\nprint(\"=\" * 70)\nprint(\"ENVIRONMENT SETUP\")\nprint(\"=\" * 70)\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(\"=\" * 70)\nprint(\"\\nâœ“ Setup complete! Ready to proceed.\")","metadata":{"_uuid":"c61c853a-7c02-45bf-adeb-7c96f051f4e2","_cell_guid":"48045e18-9aa9-4675-8260-cd9d9c2930e4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:19:49.927752Z","iopub.execute_input":"2025-11-16T18:19:49.928180Z","iopub.status.idle":"2025-11-16T18:20:15.838322Z","shell.execute_reply.started":"2025-11-16T18:19:49.928157Z","shell.execute_reply":"2025-11-16T18:20:15.837441Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 2: Load and Preprocess MNIST Dataset\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"DATA LOADING AND PREPROCESSING\")\nprint(\"=\" * 70)\n\n# Load the MNIST dataset\nprint(\"\\n[Step 1] Loading MNIST Dataset...\")\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nprint(f\"âœ“ Training samples: {x_train.shape[0]}\")\nprint(f\"âœ“ Test samples: {x_test.shape[0]}\")\nprint(f\"âœ“ Image shape: {x_train.shape[1:]}\")\n\n# Preprocess the data\nprint(\"\\n[Step 2] Preprocessing data...\")\n\n# Normalize pixel values to [0, 1]\nx_train = x_train.astype(\"float32\") / 255.0\nx_test = x_test.astype(\"float32\") / 255.0\n\n# Reshape to add channel dimension (samples, height, width, channels)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\nprint(f\"âœ“ Training data shape: {x_train.shape}\")\nprint(f\"âœ“ Test data shape: {x_test.shape}\")\n\n# Convert labels to categorical\nnum_classes = 10\ny_train_cat = keras.utils.to_categorical(y_train, num_classes)\ny_test_cat = keras.utils.to_categorical(y_test, num_classes)\n\nprint(f\"âœ“ Number of classes: {num_classes}\")\nprint(f\"âœ“ Label encoding: Categorical (one-hot)\")\n\n# Visualize sample images\nprint(\"\\n[Step 3] Visualizing sample training images...\")\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfig.suptitle('Sample Training Images from MNIST', fontsize=14, fontweight='bold')\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(x_train[i].squeeze(), cmap='gray')\n    ax.set_title(f'Label: {y_train[i]}', fontsize=10)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ“ Data preprocessing complete!\")\nprint(\"=\" * 70)","metadata":{"_uuid":"e12799f7-8f21-4c92-b282-3572093ec6f4","_cell_guid":"fa9b89cc-89fe-428c-919f-fbe70f57b205","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:20:15.839203Z","iopub.execute_input":"2025-11-16T18:20:15.839754Z","iopub.status.idle":"2025-11-16T18:20:17.050301Z","shell.execute_reply.started":"2025-11-16T18:20:15.839733Z","shell.execute_reply":"2025-11-16T18:20:17.049723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 3: Build  CNN Architecture \n\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"IMPROVED CNN MODEL V1 ARCHITECTURE\")\nprint(\"=\" * 70)\n\n# Build the Improved CNN model V1\nprint(\"\\n[Building Model] Creating Improved Convolutional Neural Network...\")\n\nmodel = keras.Sequential([\n    # ========== CONVOLUTIONAL BLOCK 1 ==========\n    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", \n                  input_shape=(28, 28, 1), padding='same', name=\"conv1_1\"),\n    layers.BatchNormalization(name=\"bn1_1\"),\n    \n    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", \n                  padding='same', name=\"conv1_2\"),\n    layers.BatchNormalization(name=\"bn1_2\"),\n    \n    layers.MaxPooling2D(pool_size=(2, 2), name=\"pool1\"),\n    layers.Dropout(0.25, name=\"dropout1\"),\n    \n    # ========== CONVOLUTIONAL BLOCK 2 ==========\n    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", \n                  padding='same', name=\"conv2_1\"),\n    layers.BatchNormalization(name=\"bn2_1\"),\n    \n    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", \n                  padding='same', name=\"conv2_2\"),\n    layers.BatchNormalization(name=\"bn2_2\"),\n    \n    layers.MaxPooling2D(pool_size=(2, 2), name=\"pool2\"),\n    layers.Dropout(0.25, name=\"dropout2\"),\n    \n    # ========== CONVOLUTIONAL BLOCK 3 (NEW!) ==========\n    layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", \n                  padding='same', name=\"conv3_1\"),\n    layers.BatchNormalization(name=\"bn3_1\"),\n    \n    layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", \n                  padding='same', name=\"conv3_2\"),\n    layers.BatchNormalization(name=\"bn3_2\"),\n    \n    layers.Dropout(0.25, name=\"dropout3\"),\n    \n    # ========== DENSE LAYERS ==========\n    layers.Flatten(name=\"flatten\"),\n    \n    layers.Dense(256, activation=\"relu\", name=\"dense1\"),\n    layers.BatchNormalization(name=\"bn_dense1\"),\n    layers.Dropout(0.5, name=\"dropout_dense1\"),\n    \n    layers.Dense(128, activation=\"relu\", name=\"dense2\"),\n    layers.BatchNormalization(name=\"bn_dense2\"),\n    layers.Dropout(0.5, name=\"dropout_dense2\"),\n    \n    layers.Dense(num_classes, activation=\"softmax\", name=\"output\")\n], name=\"Improved_MNIST_CNN_V1\")\n\n# Display model architecture\nprint(\"\\n[Model Summary]\")\nmodel.summary()\n\n# Compile the model\nprint(\"\\n[Compiling Model]\")\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nprint(\"\\nâœ“ Improved Model V1 architecture created successfully!\")\nprint(f\"âœ“ Total parameters: {model.count_params():,}\")\nprint(\"\\nðŸŽ¯ Key Improvements:\")\nprint(\"  â€¢ Added 3rd convolutional block (128 filters)\")\nprint(\"  â€¢ Batch Normalization after each layer\")\nprint(\"  â€¢ Two Conv layers per block for deeper feature extraction\")\nprint(\"  â€¢ Increased dense layer capacity (256 â†’ 128)\")\nprint(\"=\" * 70)","metadata":{"_uuid":"3f1f5030-8ce9-494c-b04a-4e7a5a7a0114","_cell_guid":"113af6bd-84fd-4108-86be-1c1d87f7de7c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:20:17.052010Z","iopub.execute_input":"2025-11-16T18:20:17.052224Z","iopub.status.idle":"2025-11-16T18:20:19.265460Z","shell.execute_reply.started":"2025-11-16T18:20:17.052207Z","shell.execute_reply":"2025-11-16T18:20:19.264876Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 4: Train the CNN Model with Callbacks\n\"\"\"\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nprint(\"=\" * 70)\nprint(\"MODEL TRAINING WITH CALLBACKS\")\nprint(\"=\" * 70)\n\nbatch_size = 128\nepochs = 25\n\nprint(f\"\\n[Training Configuration]\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Max Epochs: {epochs}\")\nprint(f\"Optimizer: Adam (lr=0.001)\")\nprint(f\"Loss function: Categorical Crossentropy\")\nprint(f\"Validation split: 10%\")\n\ncallbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=7,\n        restore_best_weights=True,\n        verbose=1,\n        mode='max'\n    ),\n    \n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    \n    ModelCheckpoint(\n        filepath=os.path.join(OUTPUT_DIR, 'best_model.keras'),\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nprint(\"\\n[Callbacks Enabled]\")\nprint(\"  - Early Stopping (patience=7)\")\nprint(\"  - Learning Rate Reduction (patience=3)\")\nprint(\"  - Model Checkpoint (save best)\")\n\nprint(\"\\n[Starting Training]\")\nprint(\"-\" * 70)\n\nhistory = model.fit(\n    x_train, y_train_cat,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.1,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL TRAINING COMPLETED\")\nprint(\"=\" * 70)\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\nfinal_train_loss = history.history['loss'][-1]\nfinal_val_loss = history.history['val_loss'][-1]\nactual_epochs = len(history.history['accuracy'])\n\nprint(f\"\\nTraining Summary:\")\nprint(f\"  Actual Epochs Trained: {actual_epochs} (out of {epochs} max)\")\nprint(f\"  Final Training Accuracy: {final_train_acc * 100:.2f}%\")\nprint(f\"  Final Validation Accuracy: {final_val_acc * 100:.2f}%\")\nprint(f\"  Final Training Loss: {final_train_loss:.4f}\")\nprint(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n\noverfitting_gap = (final_train_acc - final_val_acc) * 100\nif overfitting_gap > 2:\n    print(f\"\\n  Warning: Possible overfitting detected\")\n    print(f\"  Training-Validation gap: {overfitting_gap:.2f}%\")\nelse:\n    print(f\"\\n  Good generalization - Train-Val gap: {overfitting_gap:.2f}%\")\n\nprint(\"=\" * 70)","metadata":{"_uuid":"85aee9a3-60f7-4211-8b65-9a0e2317d856","_cell_guid":"1da0e802-cfcf-4701-8542-d1c768f65365","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:20:19.266314Z","iopub.execute_input":"2025-11-16T18:20:19.266622Z","iopub.status.idle":"2025-11-16T18:23:01.475477Z","shell.execute_reply.started":"2025-11-16T18:20:19.266597Z","shell.execute_reply":"2025-11-16T18:23:01.474748Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 5: Visualize Training History\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"TRAINING HISTORY VISUALIZATION\")\nprint(\"=\" * 70)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot accuracy\naxes[0].plot(history.history['accuracy'], label='Training Accuracy', \n             linewidth=2, marker='o')\naxes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', \n             linewidth=2, marker='s')\naxes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Accuracy', fontsize=12)\naxes[0].legend(loc='lower right')\naxes[0].grid(True, alpha=0.3)\n\n# Plot loss\naxes[1].plot(history.history['loss'], label='Training Loss', \n             linewidth=2, marker='o')\naxes[1].plot(history.history['val_loss'], label='Validation Loss', \n             linewidth=2, marker='s')\naxes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Loss', fontsize=12)\naxes[1].legend(loc='upper right')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nsave_path = os.path.join(OUTPUT_DIR, 'training_history_baseline.png')\nplt.savefig(save_path, dpi=150, bbox_inches='tight')\nprint(f\"âœ“ Training history saved as '{save_path}'\")\nplt.show()\n\nprint(\"\\nâœ“ Training visualization complete!\")\nprint(\"=\" * 70)","metadata":{"_uuid":"20907810-8d47-449f-8652-95559f51ce4f","_cell_guid":"42c9adbd-ced9-463a-b55d-9f18b99db32e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:01.476390Z","iopub.execute_input":"2025-11-16T18:23:01.476702Z","iopub.status.idle":"2025-11-16T18:23:02.314917Z","shell.execute_reply.started":"2025-11-16T18:23:01.476682Z","shell.execute_reply":"2025-11-16T18:23:02.314283Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 6: Evaluate Baseline Model Performance\nActivity 2: Baseline Performance Metrics\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"BASELINE MODEL PERFORMANCE EVALUATION\")\nprint(\"=\" * 70)\n\n# 1. ACCURACY AND LOSS\nprint(\"\\n[Metric 1] Computing Accuracy and Loss on Clean Test Set...\")\ntest_loss, test_accuracy = model.evaluate(x_test, y_test_cat, verbose=0)\n\nprint(f\"\\n{'=' * 50}\")\nprint(f\"BASELINE PERFORMANCE METRICS\")\nprint(f\"{'=' * 50}\")\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"{'=' * 50}\")\n\n# Generate predictions for further analysis\nprint(\"\\n[Generating Predictions]\")\ny_pred = model.predict(x_test, verbose=0)\ny_pred_classes = np.argmax(y_pred, axis=1)\nprint(f\"âœ“ Predictions generated for {len(y_test)} test samples\")\n\nprint(\"\\nâœ“ Basic evaluation complete!\")\nprint(\"=\" * 70)","metadata":{"_uuid":"8677ac2a-e822-4ecb-9f1a-b2bab50ef59a","_cell_guid":"c61f0128-d65e-490c-a0fa-52709c7d4df7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:02.315678Z","iopub.execute_input":"2025-11-16T18:23:02.315944Z","iopub.status.idle":"2025-11-16T18:23:07.068998Z","shell.execute_reply.started":"2025-11-16T18:23:02.315927Z","shell.execute_reply":"2025-11-16T18:23:07.068275Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 7: Confusion Matrix Analysis\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"CONFUSION MATRIX ANALYSIS\")\nprint(\"=\" * 70)\n\n# Generate confusion matrix\nprint(\"\\n[Metric 2] Generating Confusion Matrix...\")\ncm = confusion_matrix(y_test, y_pred_classes)\n\n# Display confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True, \n            cbar_kws={'label': 'Count'})\nplt.title('Confusion Matrix - Baseline Model', fontsize=16, fontweight='bold')\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.tight_layout()\nsave_path = os.path.join(OUTPUT_DIR, 'confusion_matrix_baseline.png')\nplt.savefig(save_path, dpi=150, bbox_inches='tight')\nprint(f\"âœ“ Confusion matrix saved as '{save_path}'\")\nplt.show()\n\n# Calculate per-class accuracy\nprint(\"\\n[Per-Class Performance]\")\nprint(\"-\" * 50)\nprint(f\"{'Digit':<10} {'Accuracy':<15} {'Misclassified':<15}\")\nprint(\"-\" * 50)\n\nper_class_accuracies = []\nfor i in range(num_classes):\n    class_total = cm[i, :].sum()\n    class_correct = cm[i, i]\n    class_accuracy = (class_correct / class_total) * 100\n    class_errors = class_total - class_correct\n    per_class_accuracies.append(class_accuracy)\n    print(f\"{i:<10} {class_accuracy:>6.2f}%{'':<8} {class_errors:>6}\")\n\nprint(\"-\" * 50)\navg_per_class_acc = np.mean(per_class_accuracies)\nprint(f\"Average per-class accuracy: {avg_per_class_acc:.2f}%\")\n\nprint(\"\\nâœ“ Confusion matrix analysis complete!\")\nprint(\"=\" * 70)","metadata":{"_uuid":"1e491bd8-be12-4a2a-b10c-ff6454c06104","_cell_guid":"4b08ebdc-f5cf-471f-abfc-1100e9c25971","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:07.069837Z","iopub.execute_input":"2025-11-16T18:23:07.070095Z","iopub.status.idle":"2025-11-16T18:23:07.922035Z","shell.execute_reply.started":"2025-11-16T18:23:07.070067Z","shell.execute_reply":"2025-11-16T18:23:07.921446Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 8: Detailed Classification Report\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"CLASSIFICATION REPORT\")\nprint(\"=\" * 70)\n\n# Generate classification report\nprint(\"\\n[Metric 3] Detailed Per-Class Metrics:\")\nprint(\"-\" * 70)\nreport = classification_report(y_test, y_pred_classes, \n                               target_names=[f\"Digit {i}\" for i in range(10)],\n                               digits=4)\nprint(report)\n\n# Get classification report as dictionary for further analysis\nreport_dict = classification_report(y_test, y_pred_classes, \n                                    target_names=[f\"Digit {i}\" for i in range(10)],\n                                    output_dict=True)\n\n# Create a DataFrame for better visualization\nmetrics_df = pd.DataFrame(report_dict).transpose()\nmetrics_df = metrics_df.round(4)\n\nprint(\"\\n[Summary Statistics]\")\nprint(f\"Overall Accuracy: {report_dict['accuracy']:.4f}\")\nprint(f\"Macro Avg Precision: {report_dict['macro avg']['precision']:.4f}\")\nprint(f\"Macro Avg Recall: {report_dict['macro avg']['recall']:.4f}\")\nprint(f\"Macro Avg F1-Score: {report_dict['macro avg']['f1-score']:.4f}\")\n\nprint(\"\\nâœ“ Classification report complete!\")\nprint(\"=\" * 70)","metadata":{"_uuid":"764bc1e4-cce7-4cfa-b871-7e53664b8308","_cell_guid":"a5a45444-e4ea-4ddf-90e8-2bff9389e3bb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:07.922827Z","iopub.execute_input":"2025-11-16T18:23:07.923096Z","iopub.status.idle":"2025-11-16T18:23:07.980538Z","shell.execute_reply.started":"2025-11-16T18:23:07.923076Z","shell.execute_reply":"2025-11-16T18:23:07.980005Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 9: Inference Time Measurement\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"INFERENCE TIME MEASUREMENT\")\nprint(\"=\" * 70)\n\n# Measure inference time for different batch sizes\nprint(\"\\n[Metric 4] Measuring Inference Time...\")\n\n# Single sample inference\nprint(\"\\n[Test 1] Single Sample Inference:\")\nsingle_sample = x_test[0:1]\nstart_time = time.time()\n_ = model.predict(single_sample, verbose=0)\nend_time = time.time()\nsingle_time = (end_time - start_time) * 1000\nprint(f\"  Time per sample: {single_time:.2f} ms\")\n\n# Batch inference (100 samples)\nprint(\"\\n[Test 2] Batch Inference (100 samples):\")\nnum_samples = 100\nbatch_data = x_test[:num_samples]\nstart_time = time.time()\n_ = model.predict(batch_data, verbose=0)\nend_time = time.time()\nbatch_time = end_time - start_time\navg_time_per_sample = (batch_time / num_samples) * 1000\nthroughput = num_samples / batch_time\n\nprint(f\"  Total time: {batch_time:.4f} seconds\")\nprint(f\"  Average time per sample: {avg_time_per_sample:.2f} ms\")\nprint(f\"  Throughput: {throughput:.2f} samples/second\")\n\n# Large batch inference (1000 samples)\nprint(\"\\n[Test 3] Large Batch Inference (1000 samples):\")\nnum_samples_large = 1000\nlarge_batch_data = x_test[:num_samples_large]\nstart_time = time.time()\n_ = model.predict(large_batch_data, verbose=0)\nend_time = time.time()\nlarge_batch_time = end_time - start_time\navg_time_large = (large_batch_time / num_samples_large) * 1000\nthroughput_large = num_samples_large / large_batch_time\n\nprint(f\"  Total time: {large_batch_time:.4f} seconds\")\nprint(f\"  Average time per sample: {avg_time_large:.2f} ms\")\nprint(f\"  Throughput: {throughput_large:.2f} samples/second\")\n\n# Summary\nprint(\"\\n\" + \"=\" * 70)\nprint(\"INFERENCE PERFORMANCE SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Single sample latency: {single_time:.2f} ms\")\nprint(f\"Batch (100) avg latency: {avg_time_per_sample:.2f} ms/sample\")\nprint(f\"Batch (1000) avg latency: {avg_time_large:.2f} ms/sample\")\nprint(f\"Maximum throughput: {throughput_large:.2f} samples/second\")\nprint(\"=\" * 70)\n\nprint(\"\\nâœ“ Inference time measurement complete!\")","metadata":{"_uuid":"d33104f7-392b-44c1-a2e9-55066e7d028f","_cell_guid":"ca680075-7d83-42d3-995d-37c895594594","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:07.982495Z","iopub.execute_input":"2025-11-16T18:23:07.982719Z","iopub.status.idle":"2025-11-16T18:23:10.344208Z","shell.execute_reply.started":"2025-11-16T18:23:07.982702Z","shell.execute_reply":"2025-11-16T18:23:10.343370Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 10: Sample Predictions Visualization\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"SAMPLE PREDICTIONS VISUALIZATION\")\nprint(\"=\" * 70)\n\n# Visualize correct and incorrect predictions\nprint(\"\\n[Visualization] Displaying sample predictions...\")\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle('Sample Predictions on Test Set', fontsize=16, fontweight='bold')\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(x_test[i].squeeze(), cmap='gray')\n    pred_label = y_pred_classes[i]\n    true_label = y_test[i]\n    confidence = y_pred[i][pred_label] * 100\n    \n    # Color code: green for correct, red for incorrect\n    color = 'green' if pred_label == true_label else 'red'\n    ax.set_title(f'Pred: {pred_label} ({confidence:.1f}%)\\nTrue: {true_label}', \n                 color=color, fontsize=10, fontweight='bold')\n    ax.axis('off')\n\nplt.tight_layout()\nsave_path = os.path.join(OUTPUT_DIR, 'sample_predictions_baseline.png')\nplt.savefig(save_path, dpi=150, bbox_inches='tight')\nprint(f\"âœ“ Sample predictions saved as '{save_path}'\")\nplt.show()\n\n# Find and display misclassified examples\nprint(\"\\n[Analysis] Finding Misclassified Examples...\")\nmisclassified_idx = np.where(y_pred_classes != y_test)[0]\nprint(f\"Total misclassified samples: {len(misclassified_idx)} out of {len(y_test)}\")\nprint(f\"Error rate: {(len(misclassified_idx)/len(y_test)*100):.2f}%\")\n\nif len(misclassified_idx) > 0:\n    print(\"\\n[Displaying First 10 Misclassifications]\")\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    fig.suptitle('Misclassified Examples', fontsize=16, fontweight='bold', color='red')\n    \n    for i, ax in enumerate(axes.flat):\n        if i < len(misclassified_idx):\n            idx = misclassified_idx[i]\n            ax.imshow(x_test[idx].squeeze(), cmap='gray')\n            pred_label = y_pred_classes[idx]\n            true_label = y_test[idx]\n            confidence = y_pred[idx][pred_label] * 100\n            ax.set_title(f'Pred: {pred_label} ({confidence:.1f}%)\\nTrue: {true_label}', \n                        color='red', fontsize=10, fontweight='bold')\n        ax.axis('off')\n    \n    plt.tight_layout()\n    save_path = os.path.join(OUTPUT_DIR, 'misclassified_examples.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"âœ“ Misclassified examples saved as '{save_path}'\")\n    plt.show()\n\nprint(\"\\nâœ“ Prediction visualization complete!\")\nprint(\"=\" * 70)","metadata":{"_uuid":"3f91cc38-08a4-40cd-8c34-66a946d7e9b9","_cell_guid":"fedeac6f-5750-4831-b90a-550f4fe04166","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:10.345088Z","iopub.execute_input":"2025-11-16T18:23:10.345348Z","iopub.status.idle":"2025-11-16T18:23:13.037580Z","shell.execute_reply.started":"2025-11-16T18:23:10.345332Z","shell.execute_reply":"2025-11-16T18:23:13.036692Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCELL 12: Final Summary and Output Files\n\"\"\"\n\nprint(\"=\" * 70)\nprint(\"PHASE 1 COMPLETE - FINAL SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\nðŸ“ Output Directory: {OUTPUT_DIR}\")\nprint(\"\\nðŸ“Š Generated Files:\")\nprint(\"-\" * 70)\n\n# List all generated files with details\nfile_list = []\nfor filename in sorted(os.listdir(OUTPUT_DIR)):\n    filepath = os.path.join(OUTPUT_DIR, filename)\n    size = os.path.getsize(filepath) / 1024  # Convert to KB\n    file_list.append({\n        'Filename': filename,\n        'Size (KB)': round(size, 2),\n        'Type': os.path.splitext(filename)[1][1:].upper()\n    })\n\ndf_files = pd.DataFrame(file_list)\nprint(df_files.to_string(index=False))\nprint(\"-\" * 70)\n\n# Key Metrics Summary\nprint(\"\\nðŸ“ˆ Key Performance Metrics:\")\nprint(\"-\" * 70)\nprint(f\"âœ“ Test Accuracy: {test_accuracy * 100:.2f}%\")\nprint(f\"âœ“ Test Loss: {test_loss:.4f}\")\nprint(f\"âœ“ Inference Time: {avg_time_per_sample:.2f} ms/sample\")\nprint(f\"âœ“ Throughput: {throughput_large:.2f} samples/second\")\nprint(f\"âœ“ Model Parameters: {model.count_params():,}\")\nprint(f\"âœ“ Misclassification Rate: {(len(misclassified_idx)/len(y_test)*100):.2f}%\")\nprint(\"-\" * 70)\n\n# Files description\nprint(\"\\nðŸ“ File Descriptions:\")\nprint(\"-\" * 70)\ndescriptions = {\n    'confusion_matrix_baseline.png': 'Visual confusion matrix showing classification patterns',\n    'training_history_baseline.png': 'Training/validation accuracy and loss curves',\n    'sample_predictions_baseline.png': 'Sample test predictions with confidence scores',\n    'misclassified_examples.png': 'Examples of misclassified digits for error analysis',\n    'baseline_performance.csv': 'Complete performance metrics in tabular format',\n    'mnist_cnn_baseline.h5': 'Trained model in HDF5 format (TensorFlow 1.x compatible)',\n    'mnist_cnn_baseline.keras': 'Trained model in Keras format (TensorFlow 2.x recommended)',\n    'model_architecture.json': 'Model architecture definition in JSON format'\n}\n\nfor filename, description in descriptions.items():\n    if filename in [f['Filename'] for f in file_list]:\n        print(f\"  â€¢ {filename}\")\n        print(f\"    {description}\")\n        print()\n\nprint(\"=\" * 70)\nprint(\"PHASE 1: MODEL CREATION AND BASELINE PERFORMANCE - COMPLETED\")\nprint(\"=\" * 70)","metadata":{"_uuid":"11644ac6-e279-4ad8-811e-bfeac611bef0","_cell_guid":"dd42fc95-7136-4e22-8d4f-fe28d3a1554f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-16T18:23:13.038573Z","iopub.execute_input":"2025-11-16T18:23:13.038872Z","iopub.status.idle":"2025-11-16T18:23:13.063609Z","shell.execute_reply.started":"2025-11-16T18:23:13.038853Z","shell.execute_reply":"2025-11-16T18:23:13.062659Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}